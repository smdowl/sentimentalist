#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Stock Market and Twitter
\end_layout

\begin_layout Author
Shaun Dowling, Matthieu Louis, Alessandro Ialongo, Andrey Levushkin
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Trying to predict the future of stock markets has been tried multiple times
 from very different scientific perspectives - whether it is by looking
 at co-integration of multiple stock prices in order to bet against their
 relationship diverging, or looking at repercusions and spread of macro-events
 information within the financial markets.
 A long standing belief is that stock markets are somehow related to the
 mood of the general population.
 More recently, with the rise of social media and user-created content on
 the internet, it is possible to create an approximate measure of people's
 impression of a company: a sentiment measure.
 This information can be useful for example for PR departments of companies
 to understand how they are perceived by the general opinion.
 However, it is interesting to test out whether this information can be
 useful to predict the future evolution of the companies' stock prices.
 
\end_layout

\begin_layout Standard
We investigate this relationship trying to predict stock prices of Dow Jones
 companies from relevant Twitter data.
\end_layout

\begin_layout Section
Project Architecture
\end_layout

\begin_layout Standard
The key objective of this project is to use information contained within
 Twitter posts to predict the mood of the markets towards certain stocks.
 Tweets contain a great deal of information, links between users, links
 to entities (either explicitly view a tag or plain text) and network effect
 via re-tweets.
 In their raw form, Tweets are not convenient for inference.
 As such we will focus most of our time distilling the information contained
 within the Tweets while ensuring all important aspects are preserved.
 This approach gives a nice layer of abstraction between the Information
 Retrieval and the Machine Learning steps in the pipeline.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Project-Pipeline"

\end_inset

 shows how we have modularised the project.
 In particular, the input the the machine learning algorithms are simply
 feature vectors that we can use for inference.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Graph_IRDM_pipeline.jpg
	scale 60

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Project Pipeline
\begin_inset CommandInset label
LatexCommand label
name "fig:Project-Pipeline"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Pipeline
\end_layout

\begin_layout Subsubsection
Feature Extraction Pipeline
\end_layout

\begin_layout Standard
Our feature extraction pipeline is where all the processing or raw tweets
 happens.
 All this work is handled in a single Hadoop job that splits the incoming
 tweets according to which day they were posted, and which company they
 reference.
 All tweets associated with a single company in one day are then processed
 by a single reduce task.
 This task applies several feature extractors (described in detail in Section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Feature-Extraction"

\end_inset

) to the set of tweets it receives and stores the result back in S3 in JSON
 format.
 Each line in the output file is our feature representation of all tweets
 associated with a company and the full time series of tweets represents
 how the opinion about a company on Twitter has changed over time.
 
\end_layout

\begin_layout Standard
In the feature extraction pipeline, we also considered global features that
 required processing of the entire data set to calculate.
 For example, we wanted to know how many retweets any particular user had
 had across all the data.
 We incorporated these features into the pipeline by running other Hadoop
 jobs to calculate the statistics we wanted, and then loaded these features
 maps into memory when we wanted to extract the features.
 High memory AWS instances were necessary for handling these types of features,
 due to the large number of tweets we were processing and the cost of disk
 reads.
\end_layout

\begin_layout Subsubsection
Machine Learning
\end_layout

\begin_layout Standard
Have produced a time series for each company we'd like to consider, we can
 then plug this data straight into any of a number of machine learning algorithm
s.
 In particular we have chosen Linear Regression, Gaussian Processed, and
 Support Vector machines as our training methods to try and learn how these
 features correspond to changes in stock price.
 
\end_layout

\begin_layout Subsection
AWS and Hadoop
\end_layout

\begin_layout Standard
Throughout this project we have made extensive use of AWS for computation
 and data storage.
 Since we weren't sure about which companies would yield the best results,
 we chose to explore the performance of our predictions on all of them,
 so as to make the best use of the data we were given.
\end_layout

\begin_layout Subsubsection
Preprocessing
\end_layout

\begin_layout Standard
Firstly, we found we were having trouble parsing the data since the format
 was not one JSON element per line.
 Hence, we uploaded the compressed data files to S3 so that we could access
 them quickly from an EC2 and used an EC2 instance to decompress and clean
 the data files before transfering the uncompressed data back to S3.
\end_layout

\begin_layout Subsubsection
Feature MapReduce
\end_layout

\begin_layout Standard
All feature extraction was performed via Hadoop on EMR so that we could
 explore the effectiveness of Hadoop in this context while also freeing
 us up from being constrained by long processing times.
 Since we can scale the Hadoop implementation virtually linearly we could
 iterate on our features much faster by handling all processing in a distributed
 fashion, increasing the number of instances we used if we wanted the job
 handled particularly quickly.
\end_layout

\begin_layout Standard
Once all jobs were complete, we then had a helper script that streamed all
 the output to split the data points up according to which company they
 related to.
\end_layout

\begin_layout Section
Feature Extraction
\begin_inset CommandInset label
LatexCommand label
name "sec:Feature-Extraction"

\end_inset


\end_layout

\begin_layout Standard
In order to provide our statistical models with information about the social
 networking world, we processed the raw Twitter data and extracted all the
 information that might be relevant to predicting the behaviour of the stock
 market.
 This information was then encoded numerically to be processed by our algorithms.
 We decided to split the feature extraction task into three main components:
 
\end_layout

\begin_layout Itemize

\series bold
Tweet features
\end_layout

\begin_layout Itemize

\series bold
Sentiment Extraction
\end_layout

\begin_layout Itemize

\series bold
Pagerank (and some other Global Features)
\end_layout

\begin_layout Subsection
Tweet Features
\end_layout

\begin_layout Standard
By tweet features we mean those features which are contained in each individual
 tweet, independent of the others.
 These are essentially all the relevant fields in the json file that encodes
 a twitter4j Status object.
 By examining our task, we considered the following local features to be
 the “relevant” ones:
\end_layout

\begin_layout Itemize
Number of times a tweet is favourited
\end_layout

\begin_layout Itemize
Number of times a tweet is retweeted
\end_layout

\begin_layout Itemize
Is a tweet favourited or not (Boolean)
\end_layout

\begin_layout Itemize
Is a tweet retweeted or not (Boolean)
\end_layout

\begin_layout Itemize
Is a tweet a retweet (Boolean)
\end_layout

\begin_layout Itemize
Map of (Word, Count) pairs for each word in the tweet's text
\end_layout

\begin_layout Standard
These features were computed for each tweet in the time-bin and then added
 up over all tweets in the time-bin (for Boolean variables False = 0 and
 True = 1).
 Then both the raw counts and the average values were kept as features.
 For the word count, an overall word count was computed for the text of
 all the tweets in the time-bin combined.
\end_layout

\begin_layout Subsection
Sentiment Extraction
\end_layout

\begin_layout Standard
Another feature that seems very appropriate when considering stock price
 change is the 
\emph on
sentiment 
\emph default
of the text used.
 In particular, you would expect a company with numerous positive tweets
 associated with it to generally have a positive outlook.
 This is under the assumptions that the people tweeting about a company
 is an representative subset of the people interested in trading the company's
 stock (or at least that they are important in some way on people deciding
 the value of the stock), and that positivity or negativity in a tweets
 is indicitive of the user's opinion of the outlook of the company.
\end_layout

\begin_layout Subsubsection
Stanford Implementation
\end_layout

\begin_layout Standard
We made use of the Standford NLP library 
\series bold
\emph on

\begin_inset Note Note
status open

\begin_layout Plain Layout

\series bold
\emph on
ref
\end_layout

\end_inset


\series default
\emph default
 to parse the text of each tweet during the feature extraction step.
 For every tweet parsed in for a particular company in a given time slice,
 we ran the text through the Stanford model to get an associated sentiment.
 This score ranged from 0 to 4 where an indifferent statement seemed to
 score around 2.
 However, from experimentation we noted than a large proportion of sentences
 where scored as 1 and thus shifted the score returned by the library by
 -1 to roughly center it on zero.
 
\end_layout

\begin_layout Subsubsection
Simple Implementation
\end_layout

\begin_layout Standard
Although the Stanford library boasts very good sentiment results, we found
 that it took about 0.1 seconds to parse an average tweet.
 This meant that when we ran the analyser on millions of tweets it became
 prohibitively slow, even when using Hadoop and parrallelising the sentiment
 analysis stage of the pipeline.
 This lead us to implement a much simpler sentiment analyser that simply
 calculates the sentiment of a tweet, 
\begin_inset Formula $S_{t}$
\end_inset

, as the average sentiment of each word
\begin_inset Formula 
\[
S\left(t\right)=\frac{1}{N\left(t\right)}\sum_{w\in t}s\left(w\right)
\]

\end_inset

where 
\begin_inset Formula $s\left(w\right)$
\end_inset

 is the sentiment of a particular word 
\begin_inset Formula $w$
\end_inset

 and 
\begin_inset Formula $N\left(t\right)$
\end_inset

 is the number of sentiment-rated words in a tweet.
 The sentiment for each word was calculated using the SentiWordNet 3.0 lexical
 resource and the normaliser 
\begin_inset Formula $N(t)$
\end_inset

 thus only counts the number of words in tweet 
\begin_inset Formula $t$
\end_inset

 which are found in SentiWordNet.
 This is because we did not want to penalise longer, mostly neutral tweets,
 containing a short, but possibly strong, sentiment loaded remark.
 
\series bold
\emph on

\begin_inset Note Note
status open

\begin_layout Plain Layout

\series bold
\emph on
To do: insert ref
\end_layout

\end_inset

 (SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and
 Opinion Mining (2010), Stefano Baccianella , Andrea Esuli , Fabrizio Sebastiani
)
\end_layout

\begin_layout Subsection
Pagerank
\end_layout

\begin_layout Standard
One of the global features we felt would be applicable was a rating of how
 important any given user is as an indication of how much of a reach any
 single user has, or how many people are exposed to what they have to say.
 Theoretically this could then give us a better indication as to which users
 should be focused on when expressing any particular sentiment about a company,
 since it is those people views that are being seen by a large number of
 people.
 In the context of Twitter, the most obvious directional connection between
 users is whether someone 
\emph on
follows 
\emph default
another user
\emph on
.

\emph default
 However, obtaining the follows of any given user is not possible using
 the Twitter API and therefor we could not us it.
 We felt the next best connection to determine a positive directional connection
 between users would be 
\emph on
retweet
\emph default
.
 
\end_layout

\begin_layout Standard
If someone decides to retweet something someone else has said then clearly
 they think it is in some sense important.
 The way the Twitter API exposes retweets is by allowing us to know that
 a post is based on a retweet of another users post, but only link that
 back to the origincal poster of the tweet i.e.
 if user A posts something that user B retweets and subsequently user C
 retweets user B's post, we will only see that user C retweeted user A's
 post.
 Although this is a reduction in information, we feel we still gather useful
 insight as to which users are influential posters.
 It may be most applicable to use the concept or retweets since this shows
 a more active use of another users opinion and as such may have a better
 signal to noise ratio.
 However, since we will not be able to draw direct comparisons between the
 two we cannot conclude either way.
\end_layout

\begin_layout Subsubsection
Method
\end_layout

\begin_layout Standard
If we abstract each user as a node in a graph, then we can consider the
 action of retweeting as a directed connection to whoever posted the original
 tweet.
 Thus we have a directed graph where incoming nodes represent other people
 expressing interest in whatever it is another user has tweeted about.
 Those nodes with a high in-degree could be considered more viable sources
 of information and, once we have abstracted the system in this way, a natural
 algorithm to infer user importance is PageRank.
 To draw a parrallel with the standard implementation, every page is replaced
 with a user and every link to another page is replaced with a user retweeting
 another users post.
 As such, a node (user) with a high page rank is a node (user) whose posts
 are retweeted by other users who themselves have posts that are often retweeted.
 We have chosen to run PageRank on the entire Twitter dataset in case any
 users overlap between the sets.
 Although we feel Page Rank may be a useful metric, we also still include
 a simple count of in-degree (how many other users have retweeted a users
 tweet) in case the added level of important people retweeting your post
 is not significant.
\end_layout

\begin_layout Subsubsection
Implementation
\end_layout

\begin_layout Standard
We implemented Pagerank in Hadoop.
 The only slight modification was that the intuition behind the random-surfer
 model doesn't translate to retweets and thus was not used in our formulation.
 The main reason we felt this was reasonable was that there are so many
 
\begin_inset Quotes eld
\end_inset

dangling nodes
\begin_inset Quotes erd
\end_inset

 that there is a significant amount of probability mass that is redistrubuted
 each iteration anyway, making any additional mass for a 
\begin_inset Quotes eld
\end_inset

random surfer
\begin_inset Quotes erd
\end_inset

 virtually redundant.
\end_layout

\begin_layout Subsubsection

\series bold
Other
\series default
 Global Features
\end_layout

\begin_layout Standard
By global features we mean features of our dataset which can only be computed
 by processing the dataset as a whole, as opposed to each individual tweet
 on its own.
 These amount to general information about the unique users whose tweets
 are contained in our dataset.
 Thus Pagerank is clearly a Global Feature, and we also implemented a MapReduce
 job to extract, for each user, the proportion of tweets that were originally
 somebody else's (
\begin_inset Quotes eld
\end_inset

retweets
\begin_inset Quotes erd
\end_inset

), and the average number of retweets on the user's own tweets.
 These, like Pagerank, also served as a measure of user 
\begin_inset Quotes eld
\end_inset

clout
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Subsubsection
Sentiment Weighted PageRank
\end_layout

\begin_layout Standard
In order to emphasise the opinion of important users, we also included a
 feature which takes the product of a user's sentiment score with the page
 rank of that user.
 We then sum this over a time bin to get a 
\emph on
sentiment weighted pagerank 
\emph default
for each company.
 The idea behind this feature is that if important users are showing strong
 opinions about a company then that sentiment should be more heavily considered
 than if a user who no one cares about shows similar sentiment.
\end_layout

\begin_layout Section
Stock Prediction
\end_layout

\begin_layout Subsection
Linear Regression
\end_layout

\begin_layout Standard
We use a least-square multi-dimensional linear regression as a simple algorithm
 to predict future prices given the features we extracted from the tweets
 for that given stock and time-frame.
 From a theoretical point of view, the Multiple Linear Regression models
 the stock price as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula $Y_{t+1}=\beta_{0}+\beta_{1}x_{t,1}+\beta_{2}x_{t,2}+...+\beta_{m}x_{t,m}+e_{t+1}$
\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $Y_{t+1}$
\end_inset

 is the next day stock price, 
\begin_inset Formula $x_{t,i}$
\end_inset

 the predictor of the 
\begin_inset Formula $i^{th}$
\end_inset

 relevant feature and and 
\begin_inset Formula $e_{t+1}$
\end_inset

 is the error in the prediction.
\end_layout

\begin_layout Standard
In the matrix form, this gives: 
\begin_inset Formula $\mathbf{Y}=\mathbf{\beta}\mathbf{X}+\mathbf{e}$
\end_inset

 .
 With 
\begin_inset Formula $X=\left(\begin{array}{ccccc}
x_{1}^{T}\\
x_{2}^{T}\\
x_{3}^{T}\\
...\\
x_{T}^{T}
\end{array}\right)$
\end_inset

 and each 
\begin_inset Formula $x_{i}=\left(\begin{array}{ccccc}
x_{i,1}\\
x_{i,2}\\
x_{i,3}\\
...\\
x_{i,m}
\end{array}\right)$
\end_inset


\begin_inset Formula $ $
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
hfill 
\backslash

\backslash
 
\backslash

\backslash

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We are learning this model by least-square method, that is 
\begin_inset Formula $\beta$
\end_inset

 is chosen such that it minimises the Residual Sums of Squares defined as
 : 
\begin_inset Formula $\sum_{t=1}^{T}\left(Y_{t}-Y_{t,pred}\right)^{2}$
\end_inset

 where 
\begin_inset Formula $Y_{t,pred}$
\end_inset

 is the predicted value by the model: 
\begin_inset Formula $Y_{t,pred}=X\beta$
\end_inset

.
 
\end_layout

\begin_layout Standard
Rewritting the Residual Sums of Squares we get:
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
hfill 
\backslash

\backslash

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\sum_{t=1}^{T}\left(Y_{t}-Y_{t,pred}\right)^{2}=(Y-X\beta)^{T}(Y-X\beta)$
\end_inset

 .
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
hfill 
\backslash

\backslash

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Differentiating with respect to 
\begin_inset Formula $\beta$
\end_inset

 and setting to zero:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
X^{T}X\beta & = & X^{T}Y\\
\beta & = & \left(X^{T}X\right)^{-1}X^{T}Y
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In order to train the model, we have to find the 
\begin_inset Formula $\beta$
\end_inset

 which solves this for the training set.
 In practice, implementing this can be relatively hard since inverting a
 matrix is computationaly 
\begin_inset Formula $O(n^{3})$
\end_inset

 operation.
 In some cases our feature vector was as long as 22,000 and so inverting
 a full square matrix with 22,000x22,000 is not feasable on our computers.
 In order to go around this problem, we can leverage the fact that our feature
 vectors are very sparse and use efficient algorithms such as Gaussian Eliminati
on or Backward-Forward elimination to solve the linear system of equation
 above.
\end_layout

\begin_layout Standard
We use the 'la4j' library to build sparse matrices and perform the required
 linear algebra operations on them.
\end_layout

\begin_layout Subsection
Support Vector Machine
\end_layout

\begin_layout Standard
Support Vector Machines are models used for both classification and regression.
 We will introduce the linear SVM for simplicity issue, but in practice
 we wil be using SVM with non-linear Kernel, which allow us to identify
 non-linear relationship in the data.
 A Linear Support Vector Machine works by finding the seperating hyperplane
 with largest distance possible from training points of the various classes.
 It then looks on which side of this hyperplane testing points exist and
 classifies then accordingly.
 Mathematically, finding the seperating hyperplane defined by its weight
 
\begin_inset Formula $\mathbf{w}$
\end_inset

 and bias 
\begin_inset Formula $b$
\end_inset

 is equivalent to the problem:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
hfill 
\backslash

\backslash

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\min\frac{1}{2}\mathbf{w^{T}}\mathbf{w}$
\end_inset

 , subject to : 
\begin_inset Formula $y^{n}(w^{T}x^{n}+b)\geq1$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
hfill 
\backslash

\backslash

\end_layout

\end_inset


\end_layout

\begin_layout Standard
This is a simple quadratic problem which can be solved efficiently.
\end_layout

\begin_layout Standard
However, in our case, we want to be able to capture more than just linear
 relationships.
 In order to do so, we must include Kernels into our hyperplane equations.
 Without going into the details, we want to minimize the following Lagrangian:
\end_layout

\begin_layout Standard
 
\begin_inset Formula $L(\alpha)=\sum_{n}\alpha^{n}-\frac{1}{2}\sum y^{n}y^{m}\alpha^{n}\alpha^{m}\left(K(x^{n},x^{m})+\frac{1}{C}\delta_{n,m}\right)$
\end_inset

 with the constrait: 
\begin_inset Formula $\sum_{n}y^{n}\alpha^{n}=0,\alpha^{n}\geq0$
\end_inset

.
\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $C$
\end_inset

 is the number of allowed mis-classified data points and 
\begin_inset Formula $K$
\end_inset

 is the Kernel we want to use.
\end_layout

\begin_layout Standard
In our implementation, we use the LibSVM library, which gives us a range
 of Kernel options for our non-linear SVM.
 From our experimentations, we decide to use a Radial basis function Kernel
 defined as : 
\begin_inset Formula $K(x,x')=exp(\gamma\mbox{|\ensuremath{|x-x'\mbox{|\ensuremath{|_{2}^{2}}}}})$
\end_inset

.
\end_layout

\begin_layout Subsection
Gaussian Processes
\end_layout

\begin_layout Section
Evaluation
\end_layout

\begin_layout Subsection
Increase/decrease
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/direction-table.png

\end_inset


\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/prediction-box.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Stock Value
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/regression-table.png

\end_inset


\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/regression-box.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Considerations and Improvements
\end_layout

\begin_layout Subsection
Feature Extraction
\end_layout

\begin_layout Subsection
Prediction
\end_layout

\end_body
\end_document
